#import "./template.typ": *
#import "./theme.typ": *
= Implementation <Implementation>

As mentioned during the Introduction we implemented both the `Rew` and the `Subterm` algorithms in Lean 4 and compared the resulting constraints.

Our implementation is closely aligned with the algorithms in @rewalgo and @subterm. To keep track of what side of a rewrite relation proof needs to unify we track a the direction mentioned in @PaperAlgo `l2r` that tells the unification which terms of a rewrite theorem $rho$ has to unify. The choice between $(<-)$ and $(->)$ is directed by the desired relation $r$ in @subterm. We also had to implement more logic for deeply nested rewrites of a function $f$ at the beginning of a application sequence. While the rewriting in Coq can usually recursively rewrite all occurances of the respective left-hand-side or right-hand-side of a rewrite theorem, it is not able to do that on functions.

In order to rewrite the function of an application in Coq we have to define the according subrelation, pointwise relation, and relation constraints. Consider a rewrite theorem $rho : r space f space g$. $f$ and $g$ are of type $alpha -> mono("Prop") -> beta -> mono("Prop")$. Rewriting $f$ to $g$ in a term $f space a space (a = a) space b$ where $a$ is of type $alpha$ and $b$ of type $beta$ the Coq algorithm provides a proof e.g. with $(<-)$ as the desired relation for $f space a space (a = a) space b <- g space a space (a = a) space b$. In our `Subterm` algorithm we would enter the path starting at line 8 in @subterm and produce the accodring proof. However, when changing the term to $f space a space (f space a space (a = a) space b) space b$ Coq currently only provides a proof for $f space a space (f space a space (a = a) space b) space b <- g space a space (f space a space (a = a) space b) space b$. We can see that only the outer $f$ is rewritten by Coqs implementation. In the special case where $r$ is equality Coq applies the leibniz-equality algorithm and replaces both occurances of $f$.

This inconsistancy in the Coq implementation is a crucial difference to the `Rew` algorithm which would rewrite all occurances of $f$ even when the relation is not equality. This is where we made some further changes to `Subterm` levering transitivity of implications (or open relations) as mentioned in @updatedalgo. As transitivity cannot be shown for a possibly non-transitive rewrite relation $r$ we must perform a subrelation inference to the desired relation passed as argument to `Subterm` immediately. When we operate on `Prop` directly the inferred relation is already transitive. In the other case we are in a nested call of the application case and thus work with a metavariable of type $mono("relation " tau)$ that we can force to be transitive later. This does not change our invariants as we always treat relations general enough so that it does not matter whether we work with metavariables of type $mono("relation " tau)$ or given instances of that type.

Once we inferred the relation to the desired transitive relation $r$ we can perform another rewrite on the updated term $u$. In the recursive call with $u$ the first occurrence was already replaced to $g$ and we follow the procedure for application arguments starting in line 13 in @subterm. This would invoke yet another recursive invokation where $f$ is again the function rewrite. This is how we can rewrite a term, for instance $f space a space (f space a space (f space a space (mono("True")) space b) space b) space b$, directly to $g space a space (g space a space (g space a space (mono("True")) space b) space b) space b$ and thus truely generate the same rewrites that `Rew` produces. The downside of this approach is that with many occurances of $f$ we generate almost as many `Subrelation` metavariables as the `Rew` algorithm would for this example. The `Transitive` instances however are trivial to solve and can even be closed during the constraint generation as all implications are transitive.

It is a common use case to perform rewrites using existing theorems such as addition communativity or `Nat.add_comm` ($forall n space m : NN, n + m = m + n$) or `Nat.right_distrib` ($forall n space m space k : NN, (n+m) dot k = n dot k + m dot k$). Both of those theorems are defined with all-quantifiers. During the execution of the constraint genration algorithm we are looking for unifications of the term $n + m$ for $rho := mono("Nat.add_comm")$ assuming a left-to-right rewrite. That means we have to go inside the $forall$-binder every time. As this information does not change during the execution we wrapped the algorithm in a reader monad containing all relevant information (the carrier relation $=$, the left-hand-side of $rho$, the right-hand-side of $rho$, the proof $rho$, and the possible metavariables for unused binder variables) about the rewriting theorem in the context.

The paper for generalised rewriting in type theory @sozeau:inria-00628904 that inspired the Coq implementation and the official Coq library for morphisms @coqmorphism mention many theorems that drastically simplify the constraints generated by the `Rew` or `Subterm` constraint generation algorithms. Both sources also rely on typeclass search. This has two advantages in Coq. Firstly, the by Haskell inspired typeclass system @casteran:hal-00702455 keeps track of all instances defined for a typeclass and allows access to those when resolving a typeclass. This is a convenient approach for users to create new instances that directly influence the proof search.

For instance one example mentioned in the paper @casteran:hal-00702455 defines a primitive version of sets where a set is just `Type` and `eqset`, the equivalence of sets, is thus a relation over set `SET` $->$ `SET` $->$ `Prop` and union is a function `Set` $->$ `Set` $->$ `Set`. If we want to leverage some theorems about sets for rewriting we generate proper constraints as part of the proof skeleton generation. By defining the typeclass instance $mono("instance union_proper" : mono("Proper") space (mono("eqset") ==> mono("eqset") ==> mono("eqset")) space mono("union"))$ anywhere in the code we assure that this instance is leveraged when a fitting `Proper` constraint is beding solved.

The other reason Coq uses its typecalss search for the algorithm is because it can be extended with arbitrary theorems and tactics. This is crucial as the type class instances mentioned in the paper are not sufficient for solving the generated problems. This is why Coq's deocumentation @coqmorphism also contains a few tactics that apply theorems in a very specific order to take a shortcut in the proof search. For instance the tactic`reflexive_proxy_tac` checks if there is a free existential variable (metavariable in Coq) of type relation and if so assigns any reflexive relation (provided by the `find_rewrite_relation` tactic) followed by a theorem that transforms a `ProperProxy` term into a `ReflexiveProxy` and eventually a `Reflexive` class.

Allthough the typeclass search in Lean is very efficient and tailored to ad-hoc polymorphism @selsam2020tabledtypeclassresolution it does not currently support arbitrary extension by custom tactics and theorems. This is also not a planned feature because it would disrupt the performance predictability because we would loose the overview of what tactics or theorems from possibly nested imports slow a typeclass resolution. For this reason we decided to develop our own extensible proof search and translated most typeclass instances and tactics from Coqs morphism library @coqmorphism to Lean theorems and Meta-tactics.

While we used aesop @aesop initially in combination with the `Rew` algorithm to solve simple goals, we combined the opetimisations in the constraint genation in `Subterm` with a proof search that focuses on backtracking beyond the currently active goal. We store all registered theorems inside a discrtree @discrtree which can efficiently store arbitrary values (theorems in our case) using a Lean expression as key. This allowed us to find theorems that can transform a given goal. In fact Lean uses this data structure for its typeclass resolution aswell @aesop.

With goals stored efficiently we solve the syntactic holes in the rewrite proof skeleton by a depth-first proof search where we apply the first theorem that matches the current goal and directly move forward with the updated goal (if not entirely solved). Only when we cannot apply any more theorems we backtrack. When a goal gets closed we proceed to the next goal. It may also happen that already solved goals are unsolved again because metavariables can be shared.
